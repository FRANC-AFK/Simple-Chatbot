# Simple AI Chatbot

A Flask-based chatbot with rule-based responses enhanced by AI using HuggingFace models. Features include intelligent caching, rate limiting, and comprehensive logging.

## âœ¨ Features

- ğŸ¤– **AI-Enhanced Responses** - Uses HuggingFace Llama 3.2 model for natural language generation
- ğŸ“ **Rule-Based Fallbacks** - Reliable responses when AI is unavailable
- âš¡ **Response Caching** - Reduces API calls and improves response time
- ğŸ”’ **Rate Limiting** - Protection against abuse (configurable)
- ğŸ“Š **Comprehensive Logging** - Track all requests and responses
- ğŸ³ **Container Ready** - Docker/Podman compatible with security best practices
- â™¿ **Accessible UI** - ARIA labels and keyboard navigation
- ğŸ”„ **Auto-Retry Logic** - Handles network failures gracefully
- â˜ï¸ **Cloud Deploy Ready** - Works with Render, Heroku, and other platforms

## ğŸš€ Quick Start

### Local Development

```bash
# Clone repository
git clone <your-repo-url>
cd simple-ai-chatbot

# Install dependencies
pip install -r requirements.txt

# Set environment variables
export HF_API_TOKEN="your_huggingface_token_here"

# Run application
python app.py
```

Visit [http://localhost:5000](http://localhost:5000)

### Docker/Podman

```bash
# Build image
podman build -t flask-chatbot .

# Run container
podman run -it --rm -p 5000:5000 \
  -e HF_API_TOKEN="your_token" \
  flask-chatbot
```

### Deploy to Render

1. Push your code to GitHub
2. Create a new **Web Service** on [Render](https://render.com)
3. Connect your repository
4. Add environment variable: `HF_API_TOKEN`
5. Deploy! ğŸ‰

## âš™ï¸ Configuration

All configuration is managed via environment variables. See [config.py](config.py) for all available options:

### Core Settings

| Variable | Default | Description |
|----------|---------|-------------|
| `HF_API_TOKEN` | - | **Required** - Your HuggingFace API token |
| `USE_AI` | `True` | Enable/disable AI-enhanced responses |
| `DEBUG` | `False` | Flask debug mode |
| `PORT` | `5000` | Server port |

### Cache Settings

| Variable | Default | Description |
|----------|---------|-------------|
| `ENABLE_CACHE` | `True` | Enable response caching |
| `CACHE_MAX_SIZE` | `100` | Maximum cached responses |
| `CACHE_TTL` | `3600` | Cache time-to-live (seconds) |

### Rate Limiting

| Variable | Default | Description |
|----------|---------|-------------|
| `RATE_LIMIT_MAX_REQUESTS` | `10` | Max requests per window |
| `RATE_LIMIT_WINDOW_SECONDS` | `60` | Rate limit window (seconds) |

### AI Settings

| Variable | Default | Description |
|----------|---------|-------------|
| `AI_MAX_TOKENS` | `64` | Maximum tokens per response |
| `AI_TIMEOUT` | `60` | API timeout (seconds) |

## ğŸ§ª Testing

Run the test suite:

```bash
# Run all tests
pytest tests/ -v

# Run with coverage
pytest tests/ -v --cov=chatbot

# Run specific test file
pytest tests/test_engine.py -v
```

## ğŸ“ Project Structure

```
simple-ai-chatbot/
â”œâ”€â”€ app.py                 # Flask application & routes
â”œâ”€â”€ config.py              # Configuration management
â”œâ”€â”€ requirements.txt       # Python dependencies
â”œâ”€â”€ Containerfile          # Container build instructions
â”œâ”€â”€ Procfile              # Render/Heroku deployment
â”œâ”€â”€ chatbot/
â”‚   â”œâ”€â”€ engine.py         # Core response logic
â”‚   â”œâ”€â”€ rules.py          # Rule definitions
â”‚   â”œâ”€â”€ nlp.py            # Text normalization
â”‚   â”œâ”€â”€ cache.py          # Response caching
â”‚   â”œâ”€â”€ rate_limiter.py   # Rate limiting
â”‚   â”œâ”€â”€ logger.py         # Logging configuration
â”‚   â””â”€â”€ ai/
â”‚       â”œâ”€â”€ generator.py  # AI response generation
â”‚       â”œâ”€â”€ hf_client.py  # HuggingFace API client
â”‚       â””â”€â”€ prompts.py    # AI prompts
â”œâ”€â”€ static/
â”‚   â”œâ”€â”€ css/style.css     # UI styles
â”‚   â””â”€â”€ js/chat.js        # Frontend logic
â”œâ”€â”€ templates/
â”‚   â””â”€â”€ index.html        # Chat interface
â””â”€â”€ tests/
    â”œâ”€â”€ test_engine.py    # Engine tests
    â”œâ”€â”€ test_cache.py     # Cache tests
    â””â”€â”€ test_rate_limiter.py  # Rate limiter tests
```

## ğŸ› ï¸ API Endpoints

### `GET /`
Main chat interface

### `POST /chat`
Send a message and receive a response

**Request:**
```json
{
  "message": "What are your hours?"
}
```

**Response:**
```json
{
  "reply": "We're open from 9 AM to 5 PM, Monday through Friday.",
  "type": "ai"
}
```

Response types: `ai`, `rule`, `cached`, `fallback`

### `GET /health`
Health check endpoint for monitoring

**Response:**
```json
{
  "status": "healthy",
  "ai_enabled": true,
  "cache_enabled": true,
  "hf_configured": true
}
```

### `GET /debug/hf`
Test HuggingFace API connectivity

## ğŸ“ Development

### Adding New Rules

Edit [chatbot/rules.py](chatbot/rules.py):

```python
RULES = {
    "my_rule": {
        "keywords": {"keyword1", "keyword2"},
        "response": "Your response here"
    }
}
```

### Customizing AI Prompts

Edit [chatbot/ai/prompts.py](chatbot/ai/prompts.py) to customize how AI enhances responses.

## ğŸ› Troubleshooting

### AI responses not working
1. Verify `HF_API_TOKEN` is set: `echo $HF_API_TOKEN`
2. Test API: `curl http://localhost:5000/debug/hf`
3. Check logs for error messages

### Rate limit errors
- Increase `RATE_LIMIT_MAX_REQUESTS` environment variable
- Or increase `RATE_LIMIT_WINDOW_SECONDS`

### Container won't start
- Ensure port 5000 is available
- Check container logs: `podman logs <container-id>`

## ğŸ“„ License

MIT License - Feel free to use this project for learning or production!

## ğŸ¤ Contributing

Contributions welcome! Please:
1. Fork the repository
2. Create a feature branch
3. Add tests for new features
4. Submit a pull request

## ğŸ”— Resources

- [HuggingFace Serverless Inference](https://huggingface.co/inference-api)
- [Flask Documentation](https://flask.palletsprojects.com/)
- [Render Deployment Guide](https://render.com/docs)
